---
layout: post
title:  "2020-03-14阅读笔记"
categories: 阅读笔记
tags:  深度学习 模型可解释性
author: GarrettLee
---

* content
{:toc}


## 模型可解释性

模型的可解释性对于我们探寻规律，找到相应的知识具有十分重要的作用。用户最不友好的多层神经网络模型为例，模型产生决策的依据是什么呢？大概是以比如 1/(e^-(2*1/(e^(-(2*x+y))+1) + 3*1/(e^(-(8*x+5*y))+1))+1) 是否大于 0.5 为标准（这已经是最简单的模型结构了），这一连串的非线性函数的叠加公式让人难以直接理解神经网络的「脑回路」，所以深度神经网络习惯性被大家认为是黑箱模型。[引用来自https://baijiahao.baidu.com/s?id=1601872661947976767&wfr=spider&for=pc]；

已有的研究认为所有的深度学习在本质上的突破只不过是曲线的拟合，其并不一定可以揭示到底是为什么会产生这个问题。如，教育技术中，对某个学生的测评显示该学生在未来学习中很可能会有学业失败的情况，但是，是从哪些指标中认为他未来学业失败呢？深度学习模型通常无法给我们提供一个参考意见。

文章博主还列举对抗样本的案例说明这个问题，通过对熊猫样本加入一些噪声后，会有99.3%的可能性会神经网络判定为“长臂猿”。因此人们迫切想知道神经网络的脑回路究竟是什么，为什么会将此判定为“长臂猿”？这就是由于深度学习模型不具有可解释性给人们带来的困惑。也就带来了不安全性。

![Markdown](http://i1.fuimg.com/712071/6a70a3ad51bbdc8c.png)

博文随后列举了几种模型可解释性的方法：

1. 在建模之前的可解释性方法：可以通过数据预处理，数据可视化以及探索性质的数据分析（如： MMD-critic ）

2. 建立本身具备可解释性的模型：基于规则的方法（决策树）、基于单个特征的方法（线性回归）、基于实例的方法（贝叶斯实例模型）、稀疏性方法（图稀疏性的 LDA 方法）、单调性方法

3. 在建模之后使用可解释性方法对模型作出解释


## 引用

[1] https://baijiahao.baidu.com/s?id=1601872661947976767&wfr=spider&for=pc

## 声明

作为一名**非科班出身**的教育技术学研究生，**理论水品十分有限**，阅读过程中难免存在理解上的**偏差**，还请各位同仁**批评指正**。目前我正在从事**深度学习**以及**自然语言处理**和教育领域结合的研究，感兴趣的同学可以和我交流，互相学习。微信公众号：SMNLP。